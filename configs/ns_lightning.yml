# Modifications for PCFM © 2025 Pengfei Cai (Learning Matter @ MIT) and Utkarsh (Julia Lab @ MIT), licensed under the MIT License.
# Original portions © Amazon.com, Inc. or its affiliates, licensed under the Apache License 2.0.
# we follow a similar set-up as Cheng et al. for Navier-Stokes FFM model pretraining 
train:
  seed: 42
  batch_size: 24
  max_steps: 500_000 
  limit_val_batches: 32
  val_check_interval: 500
  log_every_n_steps: 100
  gradient_clip_val: 10.
  lr_warmup_steps: 1000
  optimizer:
    type: adamw
    lr: 3.e-4
    weight_decay: 0.
    beta1: 0.9
    beta2: 0.999
  scheduler:
    type: plateau
    factor: 0.8
    patience: 10
    min_lr: 3.e-5

datasets:
  type: ns
  root: datasets/data
  train:
    data_file: ns_nw100_nf100_s64_t50_mu0.001.h5
  test:
    data_file: ns_nw10_nf100_s64_t50_mu0.001.h5

model:
  kernel: randn
encoder:
  type: fno
  n_modes: [ 16, 16, 16 ]
  emb_channels: 16
  hidden_channels: 32
  proj_channels: 256
  n_layers: 2

n_sample: 2
n_eval: 1
sample_dims: [ 32, 32, 25 ]
vis:
  downsample: 1
  vmin: -2.5
  vmax: 2.5
